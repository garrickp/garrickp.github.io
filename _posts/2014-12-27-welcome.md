---
layout: post
title: A Welcome, and a Day in the Life
---

<div class="message">
  My name is Garrick, and I am here to write about my experiences while writing code as a DevOps developer.
</div>

## When you say DevOps, what exactly do you mean?

To say that the definition (and very existence) of the DevOps role is hotly contested (not to mention its presence as one of the latest buzzwords in our industry) is like saying that lisp is a niche language. Unkind, but no less true for the unkindness. Let us start with the definition as it applies to the career and niche I have made for myself.

In two words, DevOps is meta automation. Not just "write Chef/Puppet/Ansible scripts" automation; most operations folks of any stripe are constantly busy attempting to automate themselves out of a job, yet they are still always finding more work to do. No, the automation I work on the next higher level of abstraction; I am a member of the operations team for the perspective and access, but I spend little time researching issues and no time responding to pages. Instead, I work on creating, installing, and configuring high level automation tools around, and similar to, Graphite, Logstash, Collectd, Nagios, Skyline, Docker, and more.

As a quick aside, it seems like I just tried to place myself on quite the lofty pedestal there, however while the tools I write are nowhere near as original and lack the broad applicability of these fantastic tools, they still exist on the same level of abstraction. I have yet to write a Nagios replacement, but I have written a centralized notification system around Nagios specialized for a remote DBA team. I did not write Graphite, but I have started work on a tool for the automated evaluation of long term trends and smart alerting based on those trends. I have not contributed directly to Ansible, but I have built automation around Ansible in such a way that a small operations team was able to manage package and configurations on 200+ database servers in 45 individual customer networks with a simple HTML form.

I also spend a lot of my time teaching developers, PMs, and management the value of using tools and processes like these; it surprises me to this day how many developers want to keep absolute control of their software, from dev to Production (even at 3am). I find that it is my duty to show them how much more coding they could do if they gave up control of some of these environments and tasks which have nothing to do with the development process. That said, it is still my responsibility to listen to them and give them the right tools to help them properly support their Production environment until they are ready to (or forced to) give up those reigns.

## A day in the life of...

Want a bit more? How about a quick peek into my professional life? How about today? As a warning, I end up dropping the names of a lot of popular tools; my intent is not to sell you on those tools, but rather to give an idea of what it is I work with on a daily basis. This will help you understand my biases, and understanding which will give my ramblings context.

### Refreshments and a 10,000" view

My morning starts out as I drink tea (I have become a real snob, much to my wife's delight) and start to pour over two separate Graphite dashboards. The first dashboard is a high level overview of our monitoring host, which shows me when and where we experienced high load over the last 7 days, and which of our monitoring systems caused the load. In this case, there was a huge peak that appeared in the memory usage of actual Graphite web process itself. Turns out the Graphite Django application decided to consume approximately 4 GB of the system's memory. While not a huge deal since it dropped that memory less than 5 minutes later, it did cause the kernel to give up a large chunk of disk cache, which might have an impact on our MySQL or carbon-cache processes later. I dig into the cause using Kibana over the top of Elasticsearch, and identify the cause as a few 30 days graphs which touched a large number of whisper archives. A problem for another day (though as carbon-cache and graphite are constantly pummling our data partition with about 1.4k iops, that day is not so far off), so off to a research ticket with that one.

Next is the API graphite dashboard, which monitors output of the production APIs. A few sharp (but momentary) dips in the response size and spikes in the response time catch my eye, even though the HTTP return codes all look normal. I note the times of the spikes and let my Operations colleague of the times these occurred at for research with the API owner. Thankfully, the processing time for our automated mail marking has dropped below one minute per run; Gmail must have forgiven us our initial run through the first 100k+ emails. We should be able to run that more than once every 10 minutes now, and get more timely notifications of messages not appearing in our customer's inboxes. Another ticket is opened to revisit the problem and ensure that our processing times have truly plateaued, and to reduce the processing interval. Maybe 2 minutes? Maybe, though we might have to institute process-level locks so if we are given punitive latency again due to a bug or human error we do not start encountering nasty race conditions.

### Tickets & Research

Next I dig through our ticketing system for the next tasks on our development schedule. The top two tasks involve investigating Riemann and Skyline to do trend alerting, but after the headache and swearing match I had at Riemann the previous couple of days, that particular task is going to be put on hold. So, on to Skyline it is.

Pulling up the github repo shows me that it is mostly written in Python, so I dig through the code to see how it decides to solve the problem. The answer is with two specialized libraries written in C for accelerating mathematical operations, and a custom storage engine built atop Redis which is being fed from a carbon relay. My first question comes to mind - why so many unique dependencies? The answer to that is simple and twofold: Python is slow, and the whisper storage engine that backs Graphite is - while simple to understand and elegant - stupidly inefficient.

The thought of adding yet another five or six Python dependencies to our environment is aggravating (particularly since they require build tools if you pull them using pip or easy_install), but not terrible. Installing Redis would be a waste, since we are already using Memcache to handle Graphite and web environment caching; re-writing that code would require no small amount of time (and potentially patching Graphite since it requires the use of an out-of-date version of Django). Having to edit the Skyline code to change thresholds and add new statistical models also irks at me, particularly since we would be iterating over these values quickly, and using a configuration file would be a lot more sane than writing patches (or maintaining our own Skyline fork).

A bad idea springs to mind: re-invent the wheel, in Go. The algorithms that Skyline is using are not that conceptually difficult to reproduce (not to mention that the anomaly detection in time series data receives quite a bit of theoretical attention; a fair bit of the resulting research is freely available across the web), and I could re-use the existing (albeit inefficient) storage engine and get a free efficiency boost when we migrate to a stronger storage engine, so long as I can get the raw values from the front end like I can now.

### Following through with a bad idea

And now I'm committed, even if only to a prototype.

Between handling questions about the production environment and giving suggestions on how to configure some new office-based monitoring hardware, I am able to knock out a quick proof of concept in Python. So far, so good; Python's libraries are allowing me to iterate quickly, though I immediately hit the problem with performing calculations over thousands of data points. I follow that with a better (and significantly faster) proof of concept in Go. I have won some basic threshold monitoring fed by the standard Graphite front end (obtained with the same URL that generates a image, with the incantation `&format=json` appended), with the framework to implement anomaly detection algorithms in about a day; I would call that a win. I spend a few minutes wondering if I can convince my boss, and his bosses, to let me post it under an OSS license. Much to my chagrin, it is pretty unlikely, though I will give it a shot anyways. The framework is in place and begging for a first pass at an actual forecast algorithm, but now the day is nearly over. 

Another glance over the Graphite dashboards shows nothing really new has occurred. The load and CPU utilization shows a higher normal during the day while people have our dashboards and uptime web pages, but the AWS instance is hardly breaking a sweat. Icinga2 has only sent two alerts, and both were actionable, so nothing needed to change there. I shut down my Vagrant development instances, close the lid of my computer, and go home.

## What I will be writing about

Mostly technologies I work with (including the issues I encounter while working with them and the workarounds I have Googled), as well as my thoughts on new technologies. Occasionally I plan to dive into slightly more existential discussions about the software development industry in general, and DevOps in general (as it exists for me).

I look forward to having you along for the ride!

